{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "from gensim import downloader\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "tokenize_model_1 = downloader.load('glove-wiki-gigaword-100')\n",
    "tokenize_model_2 = downloader.load('word2vec-google-news-300')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Pre-processing and tokenization\n",
    "def load_data(path_to_data):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    pos = []\n",
    "\n",
    "    # read data\n",
    "    with open(path_to_data, 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    # split data\n",
    "    for line in data:\n",
    "        if line != '\\n':\n",
    "            line_data = line.split('\\t')\n",
    "            # take only indexes 0,1,3,6\n",
    "            word  = [int(line_data[0]), line_data[1], line_data[3], int(line_data[6])]\n",
    "            pos.append(word[2])\n",
    "            sentence.append(word)\n",
    "        else:\n",
    "            sentences.append([[0,\"~\",\"ROOT\",0]] + sentence)\n",
    "            sentence = []\n",
    "    return sentences, list(set(pos + [\"ROOT\"]))\n",
    "\n",
    "\n",
    "def pos_to_oneHot(pos_list):\n",
    "    tensor_dim = len(pos_list)\n",
    "    pos_to_vec = {}\n",
    "    for pos in pos_list:\n",
    "        one_hot_tensor = torch.zeros(tensor_dim)\n",
    "        one_hot_tensor[pos_list.index(pos)] = 1\n",
    "        pos_to_vec[pos] = one_hot_tensor\n",
    "\n",
    "    return pos_to_vec\n",
    "\n",
    "\n",
    "def tokenize(sentences, glove_model,w2v_model, glove_length, w2v_length, pos_list):\n",
    "    pos_to_vec = pos_to_oneHot(pos_list)\n",
    "    set_data = []\n",
    "    tokenized_sen = []\n",
    "    counter_zero = 0\n",
    "    counter_all = 0\n",
    "    flag_glove = True\n",
    "    flag_w2v = True\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            counter_all += 1\n",
    "            if word[1] not in glove_model.key_to_index:\n",
    "                flag_glove = False\n",
    "                word_vec_1 = torch.zeros(glove_length)\n",
    "            else:\n",
    "                word_vec_1 = torch.Tensor(glove_model[word[1]].tolist())\n",
    "            if word[1] not in w2v_model.key_to_index:\n",
    "                flag_w2v = False\n",
    "                word_vec_2 = torch.zeros(w2v_length)\n",
    "            else:\n",
    "                word_vec_2 = torch.Tensor(w2v_model[word[1]].tolist())\n",
    "            if not flag_glove and not flag_w2v:\n",
    "                counter_zero += 1\n",
    "\n",
    "            word_vec = torch.cat((word_vec_1, word_vec_2))\n",
    "            pos_vec = pos_to_vec[word[2]]\n",
    "            final_vec = torch.cat((word_vec, pos_vec))\n",
    "            tokenized_sen.append(final_vec)\n",
    "            flag_glove = True\n",
    "            flag_w2v = True\n",
    "\n",
    "        set_data.append(torch.stack(tokenized_sen))\n",
    "        tokenized_sen = []\n",
    "\n",
    "    print(f\"managed to tokenize {(1-(counter_zero/counter_all))*100} of the data\")\n",
    "    return set_data\n",
    "\n",
    "def build_headers_target(sentence):\n",
    "    target = [0]*len(sentence)\n",
    "    for i in range(len(sentence)):\n",
    "        target[i] = sentence[i][3]\n",
    "\n",
    "    return torch.Tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [
    "class scoring_nn(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(scoring_nn, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size=125, num_layers=4, bidirectional=True, dropout=0.1)\n",
    "        self.scoring = nn.Sequential(\n",
    "            nn.Linear(500, 125),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(125, 1)\n",
    "        )\n",
    "\n",
    "    def init_first_hidden(self):\n",
    "        return torch.zeros(8, 1, 125), torch.zeros(8, 1, 125)\n",
    "\n",
    "    def possible_headers(self, sentence, current_index):\n",
    "        all_pairs = []\n",
    "        for i in range(len(sentence)):\n",
    "            concat_vec = torch.cat((sentence[i], sentence[current_index]), dim=1)\n",
    "            concat_vec = concat_vec.view(-1)\n",
    "            all_pairs.append(concat_vec)\n",
    "        return torch.stack(all_pairs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x.unsqueeze(1)\n",
    "        h0, c0 = self.init_first_hidden()\n",
    "        out, _ = self.lstm(input, (h0, c0))\n",
    "        scoring_mat = torch.zeros(len(x), len(x))\n",
    "        for i in range(len(out)):\n",
    "            possible_headers = self.possible_headers(out, i)\n",
    "            scores = []\n",
    "            for concat_vec in possible_headers:\n",
    "                score = self.scoring(concat_vec).tolist()[0]\n",
    "                scores.append(score)\n",
    "            scores_tensor = torch.Tensor(scores)\n",
    "            scoring_mat[i] = F.log_softmax(scores_tensor, dim=0)\n",
    "\n",
    "        scoring_mat.fill_diagonal_(0)\n",
    "        return scoring_mat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [],
   "source": [
    "def train_model(model, train_data, original_sentences, epochs):\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for i, sentence in enumerate(train_data):\n",
    "            labels = build_headers_target(original_sentences[i])\n",
    "            optimizer.zero_grad()\n",
    "            scores_mat = model(sentence)\n",
    "            loss = criterion(scores_mat, labels.long())\n",
    "            print(f\"loss: {loss}\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"epoch {epoch} loss: {loss.item()}\")\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_sentences, pos_train = load_data('train.labeled')\n",
    "    test_sentences, pos_test = load_data('test.labeled')\n",
    "    tokenized_train = tokenize(train_sentences, tokenize_model_1, tokenize_model_2, 100,300, pos_train)\n",
    "    tokenized_test = tokenize(test_sentences, tokenize_model_1, tokenize_model_2, 100,300, pos_test)\n",
    "    model = scoring_nn(tokenized_train[0][0].shape[0])\n",
    "    model = train_model(model, tokenized_train,train_sentences, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "managed to tokenize 98.79614127401737 of the data\n",
      "managed to tokenize 99.12734452122409 of the data\n",
      "loss: -0.04985513910651207\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[263], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[189], line 7\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m tokenized_test \u001B[38;5;241m=\u001B[39m tokenize(test_sentences, tokenize_model_1, tokenize_model_2, \u001B[38;5;241m100\u001B[39m,\u001B[38;5;241m300\u001B[39m, pos_test)\n\u001B[0;32m      6\u001B[0m model \u001B[38;5;241m=\u001B[39m scoring_nn(tokenized_train[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m----> 7\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenized_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtrain_sentences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[262], line 12\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, train_data, original_sentences, epochs)\u001B[0m\n\u001B[0;32m     10\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(scores_mat, labels\u001B[38;5;241m.\u001B[39mlong())\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 12\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\NLP_HW3\\venv\\lib\\site-packages\\torch\\_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    487\u001B[0m     )\n\u001B[1;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\NLP_HW3\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}